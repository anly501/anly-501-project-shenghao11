---
pdf-engine: lualatex
title: decision tree
format:
  html:
    theme: yeti
    toc: true
    code-fold: true
    toc-title: Contents
    css: styles.css
execute:
  echo: true
jupyter: python3
---

# using decision tree to my data
I am using decision tree to classifer and preidct the natural population growth rate. In the dataframe, you can see I collect the gdp, year, country, death rate, birth rate, Life expectancy and DALYs (Disability-Adjusted Life Years).  decision used to analyze data for classification and regression analysis. I can tell which factor causes the natural growth rate.
After I Re-train the decision tree using the optimal hyper-parameter 18, I achieved an accuracy of 93%, which indicates that my model predicted the natural population growth rate can be an accurate rate of 93%. 

```{python}
import pandas as pd
import seaborn as sns 
import matplotlib.pyplot as plt
from sklearn import tree
from IPython.display import Image
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix,classification_report
from sklearn import tree
from sklearn.model_selection import train_test_split
```

```{python}
population = pd.read_csv('../../data/01-modified-data/cleaned-population-py.csv')
population = population.drop(columns=['Unnamed: 0'])
population = pd.get_dummies(population,columns=['Entity'])
population.head()
```

```{python}
y = np.round(population['Natural_growth_rate'])
X=population.loc[:,population.columns !='Natural_growth_rate']
X=(X-np.mean(X,axis=0))/np.std(X,axis=0)
# INSERT CODE TO PARTITION THE DATASET INTO TRAINING AND TEST SETS
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)
```

```{python}
# INSERT CODE, AS A CONSISTENCY CHECK, TO PRINT THE TYPE AND SHAPE OF x_train, x_test, y_train, y_test
#INSERT CODE TO PARTITION DATASET INTO TRAINING-TEST
print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)
print(y.value_counts())
```

```{python}
#### INSERT CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train 
model = tree.DecisionTreeClassifier()
model = model.fit(X_train,y_train)
```

```{python}
# INSERT CODE TO USE THE MODEL TO MAKE PREDICTIONS FOR THE TRAINING AND TEST SET 
yp_train = model.predict(X_train)
yp_test = model.predict(X_test)
```

```{python}
print("------TEST------")
ConfusionMatrixDisplay.from_predictions(y_test, yp_test)
```

```{python}
# INSERT CODE TO WRITE A FUNCTION "def plot_tree(model,X,Y)" VISUALIZE THE DECISION TREE 
matrix = confusion_matrix(y_test,yp_test)
report = classification_report(y_test, yp_test,output_dict=True)
# Save the results in a data frame.
report = pd.DataFrame(report).transpose()
# display the results data frame
report
```

```{python}

def plot_tree(model,X,Y):
    fig = plt.figure(figsize=(25,20))
    _ = tree.plot_tree(model,filled=True)
    plt.show()
plot_tree(model,X,y)
```

```{python}
# COMPLETE THE FOLLOWING CODE TO LOOP OVER POSSIBLE HYPER-PARAMETERS VALUES
test_results=[]
train_results=[]

for num_layer in range(1,20):
    model = tree.DecisionTreeClassifier(max_depth=num_layer)
    model = model.fit(X_train, y_train)

    yp_train=model.predict(X_train)
    yp_test=model.predict(X_test)

    # print(y_pred.shape)
    test_results.append([num_layer,accuracy_score(y_test, yp_test),recall_score(y_test, yp_test,average='weighted')])
    train_results.append([num_layer,accuracy_score(y_train, yp_train),recall_score(y_train, yp_train,average='weighted')])
```

```{python}
test_results = np.array(test_results)
train_results = np.array(train_results)

plt.figure()
plt.scatter(train_results[:, 0], train_results[:, 1], label="training", s=20)
plt.scatter(test_results[:, 0], test_results[:, 1], label="training", s=20)
plt.plot(train_results[:, 0],train_results[:, 1])
plt.plot(test_results[:, 0],test_results[:, 1])
plt.xlabel("number of layers in decision tree (max_depth)")
plt.ylabel("Accuracy(Y=0): Traning (blue) and test (red)")
plt.show()

plt.figure()
plt.scatter(train_results[:, 0], train_results[:, 2], label="training", s=20)
plt.scatter(test_results[:, 0], test_results[:, 2], label="training", s=20)
plt.plot(train_results[:, 0],train_results[:, 2])
plt.plot(test_results[:, 0],test_results[:, 2])
plt.xlabel("number of layers in decision tree (max_depth)")
plt.ylabel("recall: Traning (blue) and test (red)")
plt.show()
```

#  Train optimal model 
  Re-train the decision tree using the optimal hyper-parameter obtained from the plot above

```{python}
#### COMPLETE THE CODE BELOW TO TRAIN A SKLEARN DECISION TREE MODEL ON x_train,y_train 
model = tree.DecisionTreeClassifier(max_depth=18)
model = model.fit(X_train, y_train)

yp_train=model.predict(X_train)
yp_test=model.predict(X_test)
plot_tree(model,X,y)

# Calculate the confusion matrix and classification report for the train and test data. 
matrix = confusion_matrix(y_test,yp_test)
report = classification_report(y_test, yp_test,output_dict=True)
# Save the results in a data frame.
report = pd.DataFrame(report).transpose()
# display the results data frame
report
```

# conclusion 
After I Re-train the decision tree using the optimal hyper-parameter 18, I achieved an accuracy of 93%, which indicates that My model can predict the natural population growth rate with an accuracy of 93%.

